{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please ensure you have watched the Chapter 2 video(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You will learn the following things in this Chapter\n",
    "\n",
    "- Basic probability rules \n",
    "- Bayes Theorem\n",
    "- How to use Python programming to estimate probabilities\n",
    "- After completing this notebook you will be able to attempt CA 1 questions 1 and 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probably useful: probability revision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to be using the concept of probability a lot in this course, so it is best\n",
    "to first review the basic ideas behind probability theory, and get used to the notation. Unfortunately, there are many notations; this course will adopt one, but we will also\n",
    "mention the others, so that you can recognise them when reading further.\n",
    "\n",
    "- An experiment results in a set of outcomes, which we will call $\\Omega$. This can be a discrete set of outcomes, such is in the classic coin toss, $\\Omega = \\{H, T\\}$, or the roll of a die, $\\Omega = \\{1, 2, 3, 4, 5, 6\\}$. However in many real life experiments, $\\Omega$, which is referred to as the *outcome space*, or *event space*, can have an infinite continuum of values. We will return to this idea later, but for the moment we will consider just discrete events to help outline the basic properties of probability.\n",
    "\n",
    "- Returning to the coin toss, we can say that a fair coin will have a probability of heads of $P(H) = 0.5$, and a probability of tails of $P(T) = 0.5$.  Each outcome of the experiment of tossing the coin,  $\\Omega = \\{H, T\\}$, are thus equally likely. Similarly, for a die roll, $\\Omega = \\{1, 2, 3, 4, 5, 6\\}$, with $p(i) = 1/6$. When an experiment has $m$ equally likely outcomes, the probability of any outcome $x$ is then:\n",
    "\n",
    "    $P(x) = \\dfrac{N(x)}{m}$\n",
    "\n",
    "    where $N(x)$ is the number of times that $x$ occurs. For example, consider the more complicated case where we toss three different coins together, a 50p, a 20p and a pound coin. The outcome space is then\n",
    "\n",
    "    $\\Omega = \\{HHH, HHT, HTH, HTT, THH, THT, TTH, TTT\\}.$\n",
    "\n",
    "    Assuming all outcomes were equally likely, then the probability of getting any one is 1/8.\n",
    "    \n",
    "- Not all outcomes are equally likely. For example, we could ask what the probability of the event that our 3-coin toss comes up with $n$ heads, so the outcome space is then  $\\Omega = \\{0, 1, 2, 3\\}$. The outcome $\\omega \\in \\Omega$ in this new experiment is just given by considering the outcomes in our previous 3-coin experiment, but ignoring which exact coin lands on Heads or Tails ie <br><br>\n",
    "\n",
    "\t- $\\omega = 1$: n = 0, corresponds to TTT\n",
    "\t- $\\omega = 2$: n = 1  corresponds to HTT, THT, or TTH\n",
    "\t- $\\omega = 3$: n = 2  corresponds to HHT, HTH, or THH\n",
    "    - $\\omega = 4$: n = 3  corresponds to HHH\n",
    "\n",
    "    Thus $P(n =0) = P(n =3) = 1/8$ while $P(n=1) = P(n=2) = 3/8$.\n",
    "  \n",
    "The above uses of $P$ hide an important aspect of probabilities: $P(x_i)$ is *normalised*, such that the sum of the probabilities of all possible events adds up to 1,\n",
    "\n",
    "$P(X) = \\sum_i^N P(x_i) = 1$,\n",
    "\n",
    "where $X = (x_i, \\ldots, x_N)$. \n",
    "\n",
    "To put this another way:\n",
    "- the probability of something which is certain to happen is 1,\n",
    "- the probability of something which is impossible to happen is 0,\n",
    "- the probability of something not happening is 1 minus the probability that it will happen.\n",
    "\n",
    "Technically this in only true for either finite or countably infinite outcome spaces. If the outcome space is truly uncountably infinite, then the definition of probability has to be relaxed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Axioms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some definitions before we continue:\n",
    "\n",
    "- **Disjoint**: Two events that cannot occur at the same time are called disjoint or mutually exclusive. \n",
    "- **Discrete** variables: this is a variable whose value can be obtained by counting, and has gaps within each value that the variable can take on. \n",
    "- **Continuous** variables:  this is a variable whose value cannot be obtained by counting, and can take on all values within the range.<br><br>\n",
    "\n",
    "For a discrete variable, if I pick any two consecutive outcomes. I cannot get an outcome that lies in between. For example, if we consider 1 and 2 as outcomes of rolling a six-sided die, then I cannot have an outcome of 1.5).\n",
    "\n",
    "The axioms of probability are:\n",
    "\n",
    "Axiom 1 : $0 \\le P(A) \\le 1$\n",
    "\n",
    "Axiom 2 : $P(\\Omega) = 1$\n",
    "\n",
    "Axiom 3 : $P(A_1 \\cup A_2 \\cup A_3 ...) = P(A_1)+P(A_2)+P(A_3)+...$\n",
    "\n",
    "Here the symbol $\\cup$ denotes **or** ie  P(event $A_1$ occurs or event $A_2$ occurs or both occur etc). This is true for disjoint events and can be used for any number of disjoint events.\n",
    "\n",
    "The axioms permit us to work out the probability of event $A$ **not** occurring,\n",
    "\n",
    "$P(A^c) = 1 - P(A)$,\n",
    "\n",
    "where $^c$ is called the **complement** ie $P$(not A). <br><br>\n",
    "\n",
    "This result can be generalised. For example, for any two events $C$, $D$, the probability of getting $C$ **or** $D$ is:\n",
    "\n",
    "$P(C \\cup D) = P(C) + P(D) - P(C \\cap D)$,\n",
    "\n",
    "where now the symbol $\\cap$ denotes **and** (also written as $P(CD)$ or $P(C, D)$ in the literature). A simple way to think of this is that the probability of getting either $C$ or $D$ is just **the sum** of the chances of getting either ($P(C) + P(D)$) minus that chances of getting both at the same time, $P(C \\cap D)$. \n",
    "\n",
    "The last part is important since we're asking for the probability of *either* $C$ **or** $D$! The best way to see this is by considering the diagram below.\n",
    "\n",
    "<img src=\"https://github.com/haleygomez/Data-Science-2024/raw/master/blended_exercises/Chapter2/union_CD_diagram.png\" width=\"400\">\n",
    "\n",
    "The orange bit in the middle is the bit we need to subtract.\n",
    "\n",
    "Note that the last expression is valid *whether or not the events are mutually exclusive (disjoint)*. That is the purpose of the **and** term, since it subtracts the probability that both the events occur at the same time. \n",
    "\n",
    "How to calculate the probability of $C$ **and** $D$ ie $P(C \\cap D)$? Well, if the events are independent (i.e. do not depend on the other event occurring), then\n",
    "\n",
    "$P(C \\cap D) = P(C) P(D)$,\n",
    "\n",
    "i.e. just **the product** of the probability of the two events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#4290C4>Example</font>\n",
    "\n",
    "Someone has a bag of M&Ms in which there are red, blue, green and orange colours.  M&Ms are picked out and replaced. Someone did this 1000 times and obtained the following results:\n",
    "\n",
    "number of blue M&Ms: 300,\n",
    "number of red M&Ms: 200,\n",
    "number of green M&Ms: 450,\n",
    "Number of orange M&Ms: 50.\n",
    "\n",
    "1. What is the probability of picking a green M&M?\n",
    "2. If there are 100 sweets in the bag, how many of them are likely to be green?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=#c38241> Solution</font>\n",
    "\n",
    "Click below to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For every 1000 M&Ms picked out, 450 are green. Therefore $P$(green) = 450/1000 = 0.45.\n",
    "\n",
    "2. The experiment suggests that 450 out of 1000 M&Ms are green. Therefore, out of 100, we would expect that 45 will be green (using ratios)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#4290C4>Example</font>\n",
    "\n",
    "All human blood can be typed as O, A, B or AB. The frequency of occurance varies dependent on group.  The probabilities for the different human blood types O, B and AB in the US are \n",
    "\n",
    "| Blood Type| Probability|\n",
    "|---|---|\n",
    "|O|0.44|\n",
    "|A|?|\n",
    "|B|0.1|\n",
    "|AB|0.04|\n",
    "\n",
    "A person in the United States is chosen at random. What is the probability of the person having blood type A?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=#c38241> Solution</font>\n",
    "\n",
    "Click below to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the four blood types O, A, B, and AB should sum to 1 we simply need to sum O, B, and AB together and the probability of type A must be what is remaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of a person at random having blood type A is 0.42\n"
     ]
    }
   ],
   "source": [
    "p_O = 0.44\n",
    "p_B = 0.1 \n",
    "p_AB =0.04\n",
    "p_A = 1. - (p_O+p_B+p_AB)\n",
    "\n",
    "print('The probability of a person at random having blood type A is {:.2f}'.format(p_A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#4290C4>Example</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the probability that a randomly chosen person does not have blood type O?              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=#c38241> Solution</font>\n",
    "\n",
    "Click below to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So we need to find out what is the the probability that a randomly chosen person does not have blood type O? Ie we need to calculate: $P ({\\rm not} O)$ or $P(O^c)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of a person at random not being able to donate blood to anyone is 0.56\n"
     ]
    }
   ],
   "source": [
    "p_not_O = 1-p_O\n",
    "print('The probability of a person at random not being able to donate blood to anyone is {:.2f}'.format(p_not_O))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the information given, we know that being a potential donor for a person with blood type A means having blood type A **or** O.\n",
    "\n",
    " We therefore need to find $P$(A or O). Since the events A and O are disjoint, we can use the addition rule for disjoint events to get:\n",
    "\n",
    " $P(A \\cup O) = P(A) + P(O) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of a person at random being able to donate to someone with type A is 0.86\n"
     ]
    }
   ],
   "source": [
    "p_A_or_O = p_A + p_O\n",
    "print('The probability of a person at random being able to donate to someone with type A is {:.2f}'.format(p_A_or_O))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have enough knowledge of the probability basics to consider *conditional probabilities*. Technically speaking, all probabilities are conditional. For example, the probability that my coin will land either heads or tails is conditioned by the probability that the coin will land. Or indeed have a heads and a tails! In a less contrived example, the probability that an astronomer is observing a specific type of star, say an A2 star, is first conditioned on the probability that what they are observing is indeed a star, and not another astronomical phenomenon. \n",
    "\n",
    "So conditional probabilities are important. They are denoted in the following way: **the probability of event $A$ given condition (or event) $B$ is $P(A|B)$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a classic example: what is the probability of randomly drawing the Queen of Spades ($QoS$) from a well-shuffled, true pack of cards? \n",
    "\n",
    "There are 52 cards in total, so the probability of drawing any card ($C$) is simply $P(C) = 1/52$. The probability of drawing our desired $QoS$ is then $P(QoS) = 1/52$. Now consider that the dealer is truthful, and tells you that the card you have just drawn is a face ($F$) card. Now what is the probability $P(QoS)$?\n",
    "\n",
    "Well, first, the probability of drawing a face card that's also the $QoS$ is given by $P(QoS \\cap F)$. But since we know that the card is a face card, our probabilities for $P(QoS)$ and $P(F)$ are wrong in the sense that they were derived by dividing by all cards - 1/52, or 12/52, since there are 12 face cards in the pack. Instead we want to renormalise our probabilities to the region of outcome space where $F$ is true, so we need to divide by $P(F)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem\n",
    "\n",
    "Bayes’ theorem, named after 18th-century British mathematician Thomas Bayes, is a mathematical formula for determining conditional probabilities. This theorem has huge importance in the field of data science. It is used in finance to rate the risk of lending money to potential borrowers. It can be used to determine the accuracy of medical test results by accounting for both the probability of a person having the disease and the accuracy of the test itself.  The famous Bayes Theorem is derived from considering the idea of conditional probabilities introduced above. \n",
    "\n",
    "Imagine for instance that someone has a cough and we want to know if this means they have lung disease. Let’s say you know: \n",
    "- the probability of somebody having a cough *given* that they have lung disease $X$ ie **$P$(cough|lung disease)**, \n",
    "- the probability of somebody in general having lung disease $X$ ie **$P$(lung disease)**,\n",
    "- the probability of somebody in general having a cough ie **$P$(cough)**. \n",
    "\n",
    "With these 3 pieces of information you should be able to calculate the probability of somebody having lung disease $X$ *given* that they have a cough ie **$P$(lung disease|cough)** - but how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard form of Bayes Theorem\n",
    "\n",
    "Starting from the equations above, given that $P(A \\cap B) = P(B \\cap A)$ and two independent events, then we can write\n",
    "\n",
    "$P(A | B) P(B)  = P(B | A) P(A)$ \n",
    "\n",
    "such that,\n",
    "\n",
    "$P(A | B)   = \\dfrac{P(B | A) P(A)}{P(B)}$,\n",
    "\n",
    "which is the standard form of Bayes Theorem. \n",
    "\n",
    "A simple way to visualise this result is to consider the areas in the figure below.\n",
    "\n",
    "<img src=\"https://github.com/haleygomez/Data-Science-2024/raw/master/blended_exercises/Chapter2/ProbAgivenB_diagram.png\" width=\"400\">\n",
    "\n",
    "We can think of the denominator as re-normalising the probability into a section of probability space in which B has occured.\n",
    "\n",
    "One can generalise the denominator by considering that,\n",
    "\n",
    "$P(B) = P(B \\cap A) + P(B \\cap A^c) = P(B | A)P(A) + P(B|A^c)P(A^c)$\n",
    "\n",
    "to give the result\n",
    "\n",
    "$P(A | B)   = \\dfrac{P(B | A) P(A)} {  P(B | A)P(A) + P(B|A^c)P(A^c) }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes with Models and Datasets\n",
    "\n",
    "Bayes Rule is normally used to determine the probability of a specific model, $\\theta$, given some data D, such that\n",
    "\n",
    "$P(\\theta|D) = \\dfrac{P(D|\\theta) P(\\theta)}{P(D)}$ where\n",
    "\n",
    "where $P(D|\\theta)$ is the *likelihood*, $P(\\theta)$ is the *prior*, and $P(D)$ is the *evidence*. $P(\\theta|D)$ is the *posterior*. \n",
    "\n",
    "The standard way to write Bayes Rule is then,\n",
    "\n",
    "$P(\\theta | D) = \\dfrac{ P(D | \\theta) P(\\theta)} { P(D)}$\n",
    "\n",
    "Let's define the terms:\n",
    "\n",
    "- $P(\\theta | D)$ the posterior - the probability of model parameter $\\theta$ being true, given the data\n",
    "- $ P(D | \\theta) $ the likelihood - given model parameter $\\theta$ what is likelihood of obtaining the data\n",
    "- $ P(\\theta)$ the prior - the probability of the model parameter $\\theta$ being ‘true’\n",
    "- $P(D)$ the evidence - the probability of getting the data, give all possible model parameter values ($\\theta$ and others!)\n",
    "\n",
    "<img src=\"https://github.com/haleygomez/Data-Science-2024/raw/master/blended_exercises/Chapter2/bayes_image.png\" width=\"500\">\n",
    "\n",
    "Why is Bayes so powerful? Here's an example: if cancer is related to age, then, using Bayes’ theorem, a person's age can be used to more accurately assess the probability that they have cancer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priors\n",
    "\n",
    "People feel uncomfortable about ‘priors’, since often they are a ‘best guess’. Indeed, different analysts may have differing opinions about what the prior for a given experiment should be.  Although ‘frequentists’ disagree with the use of priors, note that technically they do assume one: they assume that all is equally likely (i.e. a ‘flat’ prior).\n",
    "\n",
    "Clearly this is also wrong.  So priors are useful — but they must be clearly stated. They provide a formal means for the analyst to include previous information that is relevant to the experiment. They also allow you test whether the model is good.   \n",
    "\n",
    "Where we have new data in an experiment, we can reapply Bayes Rule to get a new posterior. But what to use for the prior? Well, the posterior from the previous analysis! Hence the statement:\n",
    "\n",
    "Yesterdays posterior is tomorrow's prior\n",
    "\n",
    "This feedback loop makes Bayes Theorem particularly useful in machine learning, in which decision making needs to adapt to new information as it comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#4290C4>Example</font>\n",
    "\n",
    "Imagine that a box contains five coins, one of which is a joke (J) coin, with heads on both sides. A coin is selected at random from the box, and flipped 3 times. The result is 3 heads (3H). What is probability that the coin is the trick coin?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=#c38241> Solution</font>\n",
    "\n",
    "Click below to see the Solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we should define what we are trying to work out. \n",
    "\n",
    "We are interested in $P(J | 3H)$. We will let the normal coin be denoted by $C$. So using Bayes Theorem we can write,\n",
    "\n",
    "$P(J | 3H) = \\dfrac{ P(3H | J) P(J)}  {P(3H) }$\n",
    "\n",
    "To get the probability of $P(3H)$ we need to add up all possibilities of getting it.\n",
    "\n",
    "$P(J | 3H) = \\dfrac{ P(3H | J) P(J)}  {P(3H | J) P(J) + P(3H | C) P(C)}$\n",
    "\n",
    "The probability of randomly selecting the joke coin is $P(J) = 1/5$. \n",
    "\n",
    "The probability of not selecting it, is $P(J^c) = 1 - 1/5 = 4/5 = P(C)$. \n",
    "\n",
    "The probability of getting 3 heads with the joke coin is 1, so \n",
    "\n",
    "$P(3H | J) = 1$\n",
    "\n",
    "The probability of getting 3 heads with a standard coin is $(1/2) \\times (1/2) \\times (1/2)$ (remember these are independent events), so\n",
    "\n",
    "$P(3H | C) = 1/8$\n",
    "\n",
    "$P(J | 3H) = \\dfrac{ 1 \\times 1/5}  {1 \\times 1/5 ~ + ~1/8 \\times 4/5} = 2 / 3$\n",
    "\n",
    "So there's a 66% chance the coin that we are seeing flipped is the joke coin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#4290C4>Example</font>\n",
    "\n",
    "A couple has 2 children and the older child is a boy. If the probabilities of having a boy or a girl are both 50%, what's the probability that the couple has two boys?\n",
    "\n",
    "We already know that the older child is a boy. The probability of two boys then is equivalent to the probability that the younger child is a boy, which is 50%.  Show this is indeed true using Bayes Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=#c38241> Solution</font>\n",
    "\n",
    "Click below to see the Solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Bayes Theory let's evaluate this formally: \n",
    "\n",
    "Define the events, $A$ and $B$ as follows:\n",
    "\n",
    "- $A  = \\mbox{both children are boys}$\n",
    "- $B = \\mbox{the older child is a boy}$\n",
    "\n",
    "So we need to find out\n",
    "\n",
    "$P(A | B) = \\dfrac{P(B | A)P(A)}{P(B)}$ where\n",
    "\n",
    "$P(B) = \\dfrac{1}{2}$ (the probability of the older child being a boy)\n",
    "\n",
    "$P(A) = \\dfrac{1}{4}$ (probability of both children being boys - this is an **and** probability - $=0.5 \\times 0.5 $).\n",
    "\n",
    "$P(B|A) = 1$ (the probability of the older child being a boy given both children are boys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that both children are boys given older child is a boy is 0.50\n"
     ]
    }
   ],
   "source": [
    "p_A = 1./4\n",
    "p_B = 1./2\n",
    "p_B_given_A = 1.\n",
    "\n",
    "p_A_given_B = (p_A*p_B_given_A)/p_B\n",
    "\n",
    "print('Probability that both children are boys given older child is a boy is {:.2f}'.format(p_A_given_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#4290C4>Example</font>\n",
    "\n",
    "A woman's DNA matches that of a sample found at a crime scene. The chances of a DNA match are just one in two million, the court interprets this as the chance that it came from someone else is 1 in 2 million.  \n",
    "\n",
    "Given the court concludes that the probability that she is guilty $= 1 - \\dfrac{1}{2,000,000}$ which is basically 100%, she gets sent to jail for a very long time.   \n",
    "\n",
    "The women comes from a city where approximately 400,000 women with ages greater than 18 live, approximately 300,000 of which are from a similar ethnic group. Use a simple Bayes theorem approach to show why the justification above for sending her to jail is completely wrong (this is known as the prosecutor's fallacy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=#c38241> Solution</font>\n",
    "\n",
    "Click below to see the Solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer above mistakes the one in two million for the probability of the woman's innocence. In order to assess the woman's guilt properly, we need to need to take the fact that she matched the sample as a given, and see *how much more likely this makes her to be guilty than she was before the DNA evidence came to light*.\n",
    "\n",
    "But how likely is it that the DNA match profile also exists in the population at large? Who else could there be out there with that profile?  The match probability of one in two million tells how likely it is that a random person's DNA profile will match the crime sample not how guilty that person is. \n",
    "\n",
    "To take the match probability into account we need to calculate the likelihood ie the \n",
    "\n",
    "$\\mbox{likelihood } = 2,000,000 = \\dfrac{\\mbox{Probability of observing the DNA if the defendant is GUILTY}}{\\mbox{Probability of observing the DNA if the defendant is INNOCENT}}$\n",
    "\n",
    "This tells us that the 2 million times number simply tells us how more likely we are to observe the evidence if the woman is guilty, than if she is innocent.  \n",
    "\n",
    "Bayes theorem allows us to write\n",
    "\n",
    "$\\mbox{Posterior odds of guilt} = \\mbox{likelihood} \\times \\mbox{prior}$\n",
    "\n",
    "Assuming that this woman is no more likely to be guilty than any other woman in the local area we can estimate that there is a 1 in 300,000 chance that she is guilty - this is then our prior odds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odds woman is guilty after seeing the DNA evidence is 1 in 7\n",
      "Probability woman is guilty after seeing the DNA evidence is 87%\n",
      "So there is a high probability that she is guilty of the crime given DNA evidence but not 100% sure.\n"
     ]
    }
   ],
   "source": [
    "prior = 1./3e5\n",
    "likelihood = 2e6\n",
    "\n",
    "answer_odds = likelihood*prior # let's do it in odds to compare with the 1 in 2 million number \n",
    "\n",
    "print('Odds woman is guilty after seeing the DNA evidence is 1 in {:.0f}'.format(answer_odds))\n",
    "\n",
    "answer = answer_odds/(answer_odds+1)*100\n",
    "print('Probability woman is guilty after seeing the DNA evidence is {:.0f}%'.format(answer))\n",
    "print('So there is a high probability that she is guilty of the crime given DNA evidence but not 100% sure.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian vs Frequentist:\n",
    "\n",
    "Bayesian analysis has a somewhat formidable reputation for being extremely difficult… why is that?\n",
    "\n",
    "- In general, the denominator can be difficult to evaluate\n",
    "- Tricky integrals\n",
    "- Often require numerical solutions\n",
    "- Large (multivariate) parameter space\n",
    "- In the 20th century, the development of Monte Carlo Markov Chains have made the evaluation of the integrals and the probabilities much easier. \n",
    "\n",
    "You will learn more about this later in the course!\n",
    "\n",
    "**Bayesian statistician**\n",
    "- Philosophy of science: we do not “rule out” models, just determine their probabilities \n",
    "- Argument: *“the prior probability is a logical necessity when assessing the probability of a model. It should be stated, and if it is unknown you can just use an uninformative (wide) prior”* \n",
    "\n",
    "**Frequentist statistician**\n",
    "- Philosophy of science: we attempt to “rule out” or falsify models if $P$(data) given a model is too small.\n",
    "- Argument: *“setting the prior is subjective - two experimenters could use the same data to come to two different conclusions just by taking different priors”*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To recap: how do we estimate probability?\n",
    "\n",
    "Our examples of coin flipping, die rolling and card selecting, we introduced the notion that the probability of a particular outcome or event is simply the number of times that event occurs, divided by the number of all possible outcomes.\n",
    "\n",
    "But say you have a coin, and you want to know $P(H)$ -- how do you proceed? You could guess that the coin is fair and assign 0.5 to outcome heads/tails. But is the coin fair? One way you could test this is to perform lots of experiments (coin flips) and keep track of the outcome. If you do enough of these, eventually you will get an empirical measure,\n",
    "\n",
    "$P(H) = \\dfrac{n_H}{n_{\\rm flips}}$\n",
    "\n",
    "where $n_H$ is the number of heads that appeared in the experiment and $n_{\\rm flips}$ is the number of times you flipped the coin (and counted the result). But when do you stop? Well, that depends on how accurately you want to know $P(H)$. But for now, we will simply note that this type of determination of $P$ is *frequentist*, in that the probability is defined by counting the instances of occurrence.\n",
    "\n",
    "However, what about the probability that it will rain tomorrow? You can see straight away that such a probability is more difficult to define. In fact, the use of Bayes Theorem, and in particular the prior, introduces the idea that $P$ represents the belief that something will occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to tackle the **Chapter 2 quiz** on Learning Central and the [Chapter 2 yourturn notebook](https://github.com/haleygomez/Data-Science-2024/blob/master/blended_exercises/Chapter2/Chapter2_yourturn.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
